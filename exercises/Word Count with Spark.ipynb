{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphics and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This line configures matplotlib to show figures embedded in the notebook, \n",
    "# instead of opening a new window for each figure. \n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# general graphics settings\n",
    "matplotlib.style.use('ggplot')\n",
    "matplotlib.rcParams['figure.figsize'] = (16, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This IPython notebook comes with [Spark][1] preinstalled and already initialized.  A global [SparkContext][2] is available in variable `sc`:\n",
    "\n",
    "[1]: http://spark.apache.org/\n",
    "[2]: http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f47a414b750>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Word count* using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are first going to write a simple \"word count\" script in plain Python, then rework it to use Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Read the whole corpus of Shakespeare Theatre Plays into a single long Python string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of The Complete Works of William Shakespeare, by\n",
      "William Shakespeare\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_text_file(filename):\n",
    "    return open(filename, 'r').read()\n",
    "\n",
    "text = load_text_file('/srv/nfs/datasets/shakespeare.txt')\n",
    "\n",
    "# print sample text\n",
    "print text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5465102\n"
     ]
    }
   ],
   "source": [
    "# it is a long string indeed\n",
    "print len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Now try to normalize the text:\n",
    "\n",
    "  * first convert it to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿the project gutenberg ebook of the complete works of william shakespeare, by\n",
      "william shakespeare\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text2 = text.lower()\n",
    "\n",
    "# sample first two lines\n",
    "print text2[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * then remove punctuation (except for `-`, to keep character sequences like \"Stratford-upon-Avon\" as a single word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   the project gutenberg ebook of the complete works of william shakespeare  by william shakespeare \n"
     ]
    }
   ],
   "source": [
    "# see: https://docs.python.org/2/library/re.html\n",
    "import re\n",
    "punctuation = re.compile(r'[^\\w-]', re.M)\n",
    "text3 = punctuation.sub(' ', text2)\n",
    "\n",
    "# sample first two lines\n",
    "print text3[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Now we can split into words at whitespace boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'by', 'william', 'shakespeare', 'this', 'ebook', 'is', 'for', 'the', 'use']\n"
     ]
    }
   ],
   "source": [
    "words = text3.split()\n",
    "\n",
    "print words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Finally, count words. We want to build a `count` map from words to integers; we do so by first creating an empty map (defaulting to zero for unknown values) and then incrementing the value associated to a word, each time we see a word. For a complete overview of idioms for counting things in Python, please see <http://treyhunner.com/2015/11/counting-things-in-python/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fawn', 14), ('considered-', 1), ('nunnery', 6), ('gag', 1), ('woods', 15), ('cxsar', 1), ('spiders', 4), ('hanging', 38), ('offendeth', 1), ('beadsmen', 1), ('scold', 8), ('mustachio', 2), ('mutinies', 5), ('rests-that', 1), ('out-night', 1), ('benvolio', 17), ('slothful', 1), ('appropriation', 1), ('strictest', 1), ('bringing', 17)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "counts = defaultdict(int)\n",
    "for word in words:\n",
    "    counts[word] += 1\n",
    "        \n",
    "print counts.items()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for sampling the output, print out the counts of the most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27820 14665\n"
     ]
    }
   ],
   "source": [
    "print counts['the'], counts['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the same with Spark.  You might want to read the [Spark API Overview](http://spark.apache.org/docs/latest/quick-start.html) along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Spark already provides a method to read a text file; differently from Python's `read()` function, it will be chunked into lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The Project Gutenberg EBook of The Complete Works of William Shakespeare, by', u'William Shakespeare', u'', u'This eBook is for the use of anyone anywhere at no cost and with', u'almost no restrictions whatsoever.  You may copy it, give it away or']\n"
     ]
    }
   ],
   "source": [
    "text = sc.textFile('hdfs:/data/shakespeare.txt')\n",
    "\n",
    "# sample first few items\n",
    "print text.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.1) Use Spark's [flatMap](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) RDD method to turn text lines into words. We'll fold case and remove punctuation later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The', u'Project', u'Gutenberg', u'EBook', u'of', u'The', u'Complete', u'Works', u'of', u'William']\n",
      "CPU times: user 3.16 ms, sys: 161 µs, total: 3.32 ms\n",
      "Wall time: 83.7 ms\n"
     ]
    }
   ],
   "source": [
    "def mapper(line):\n",
    "    return line.split()\n",
    "\n",
    "words = text.flatMap(mapper)\n",
    "\n",
    "# sample first few items\n",
    "print words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.2) Now convert words to lowercase; we could also apply other transformations like removing punctuation here. Notice the CPU timings; where does the actual work happen and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 3 µs, total: 22 µs\n",
      "Wall time: 26 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def lowercase(s):\n",
    "    return s.lower()\n",
    "\n",
    "words2 = words.map(lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'the', u'project', u'gutenberg', u'ebook', u'of', u'the', u'complete', u'works', u'of', u'william']\n",
      "CPU times: user 3.23 ms, sys: 0 ns, total: 3.23 ms\n",
      "Wall time: 80.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print words2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) For each word, we output a pair *(word, count)*.  For simplicity, the count is always 1. If were processing a larger fileset, it could help to output a real \"local\" count here, to limit the amount of data transferred over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 1),\n",
       " (u'project', 1),\n",
       " (u'gutenberg', 1),\n",
       " (u'ebook', 1),\n",
       " (u'of', 1),\n",
       " (u'the', 1),\n",
       " (u'complete', 1),\n",
       " (u'works', 1),\n",
       " (u'of', 1),\n",
       " (u'william', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_key_value(word):\n",
    "    return (word, 1)\n",
    "\n",
    "words3 = words2.map(to_key_value)\n",
    "\n",
    "# sample first few items\n",
    "words3.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Perform a \"reduce\" step: aggregate key/value pairs by key (i.e., word) and reduce the set of values with operation `add`.  Note that Spark assumes the operation is commutative and associative (there is no guarantee the values in the set to be reduced will come in a particular order), but it has no way of checking and/or guaranteeing it.  If you pass in a function that's not associative or commutative, you'll get weird results or errors, period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'fawn', 11),\n",
       " (u'considered,', 2),\n",
       " (u'considered.', 3),\n",
       " (u'mustachio', 1),\n",
       " (u'fleeces', 1),\n",
       " (u'woods', 8),\n",
       " (u'sending.', 3),\n",
       " (u'hanging', 27),\n",
       " (u'offendeth', 1),\n",
       " (u'dance;', 4)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "counts = words3.reduceByKey(add)\n",
    "\n",
    "# sample first few items\n",
    "counts.take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
