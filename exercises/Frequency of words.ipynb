{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: this notebook requires the _ESC403 2016 v7_ (or later) VM.**\n",
    "    \n",
    "It will *not* run with earlier versions of the VM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphics and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This line configures matplotlib to show figures embedded in the notebook, \n",
    "# instead of opening a new window for each figure. \n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# general graphics settings\n",
    "matplotlib.style.use('ggplot')\n",
    "matplotlib.rcParams['figure.figsize'] = (16, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This IPython notebook comes with [Spark][1] preinstalled and already initialized.  A global [SparkContext][2] is available in variable `sc`:\n",
    "\n",
    "[1]: http://spark.apache.org/\n",
    "[2]: http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e8253a4ca2c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# SparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however that Spark has been configured to use the *local* executor, so the level of parallelism is effectively limited by the amount of CPUs available to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to define and use a single function for computing word counts given a text file. Refer to the notebook \"Word Count with Spark\" for more details on the \"word count\" and how to do it in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see: https://docs.python.org/2/library/re.html\n",
    "import re\n",
    "punctuation = re.compile(r'[^\\w]', re.M)\n",
    "\n",
    "from operator import add\n",
    "\n",
    "def wordcount(filename):\n",
    "    # make a Spark RDD from a text file\n",
    "    lines1 = sc.textFile('hdfs://' + filename)\n",
    "    # normalize (lowercase, remove punctuation, etc.)\n",
    "    lines2 = lines1.map(lambda line: punctuation.sub(' ', line).lower())\n",
    "    # break each line into words (creates a new RDD)\n",
    "    words1 = lines2.flatMap(lambda line: line.lower().split())\n",
    "    # final map/reduce step\n",
    "    words2 = words1.map(lambda word: (word, 1))\n",
    "    counts = words2.reduceByKey(add)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the word counts for the [complete works of William Shakespeare][1] (downloaded from [Project Gutenberg][2]) in a single line:\n",
    "\n",
    "[1]: http://www.gutenberg.org/ebooks/100\n",
    "[2]: http://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc1 = wordcount('/data/shakespeare.txt')\n",
    "\n",
    "wc1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we get *sorted* output? Most-frequently used words first?  Spark's [takeOrdered()][1] method provides a solution:\n",
    "\n",
    "[1]: http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeOrdered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc1.takeOrdered(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that `takeOrdered()` uses the normal Python ordering for values in the RDD: in this cases, 2-tuples are sorted lexicographically i.e. according to the string value of the \"word\" part first.  To sort on the *second* item in the *(word, count)* pair, we need to pass a custom `key` function.  Also, the sort order is always ascending -- so we need to flip numbers around 0 to get a descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to sort on the *second* item in a tuple, we need to pass a custom `key` function\n",
    "wc1.takeOrdered(5, lambda (k,v): -v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the word frequency in the Dickens' corpus instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc2 = wordcount('/data/dickens.txt')\n",
    "\n",
    "# show the number of words contained\n",
    "print wc2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc2.takeOrdered(5, lambda (k,v): -v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of most used words are quite similar.  How can we find word usage dissimilarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First approach:** We have two tables with a common column -- use a \"join\" operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = wc1.join(wc2)\n",
    "\n",
    "wc.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first attempt yields unexpected results though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "only_in_shakespeare = wc.filter(lambda (k, (v1, v2)): v2 == 0)\n",
    "\n",
    "only_in_shakespeare.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there *are* words which are used in Shakespeare's plays and not in Dickens' novels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Occurences of 'thou' in Dickens:\", wc2.filter(lambda (k,v): k=='thou').take(1)\n",
    "print \"Occurences of 'thou' in Shakespeare:\", wc1.filter(lambda (k,v): k=='thou').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we need an \"outer join\" instead?  (The dataset is too large for running a \"full outer join\" on the computer we're running this playbook on -- reduce it to the top 1000 words first to be tractable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reduce dataset first, to complete in a reasonable time\n",
    "def top(rdd, num=1000):\n",
    "    return sc.parallelize(rdd.top(num, lambda (k,v): v))\n",
    "\n",
    "top_wc1 = top(wc1)\n",
    "top_wc2 = top(wc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = top_wc1.fullOuterJoin(top_wc2)\n",
    "\n",
    "wc.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "only_in_shakespeare = wc.filter(lambda (k, (v1, v2)): v2 == None)\n",
    "\n",
    "only_in_shakespeare.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, PySpark already has a method [subtractByKey][1] to take the difference:\n",
    "\n",
    "[1]: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.subtractByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "only_in_dickens = top_wc2.subtractByKey(top_wc1)\n",
    "\n",
    "only_in_dickens.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second approach:** (suggested during class) \n",
    "\n",
    "- Normalize the counts, dividing by the total number of words in the corpus.  In other words, we seek percentages expressing \"usage popularity\" of a word.\n",
    "- Compare these \"usage popularity\" across the two sets; e.g., select only words that differ significantly in usage.  Here \"significantly\" could mean \"difference of usage popularity is over a certain threshold.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_by_total_count(wc):\n",
    "    tot_words = wc.count()\n",
    "    wfreq = wc.map(lambda (k,v): (k, 100.0*v/tot_words))\n",
    "    return wfreq\n",
    "\n",
    "wf1 = normalize_by_total_count(wc1)\n",
    "wf2 = normalize_by_total_count(wc2)\n",
    "\n",
    "wf = wf1.join(wf2)\n",
    "\n",
    "wf.filter(lambda (k, (v1,v2)): v1-v2 > 0.1).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Zipf's law\n",
    "\n",
    "Zipf law asserts that a word's frequency is inversely proportional to its rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_max = 1000\n",
    "\n",
    "# power law y=1/x\n",
    "y = 1.0 / np.arange(1, x_max)\n",
    "\n",
    "print y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_words_with_count1 = wc1.top(x_max, lambda (k,v): v)\n",
    "\n",
    "top_words_with_count1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_freq1 = float(top_words_with_count1[0][1])\n",
    "\n",
    "top_freq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq1 = np.array([(occurrences / top_freq1) for (word, occurrences) in top_words_with_count1])\n",
    "\n",
    "print freq1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat count with Dickens' text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_words_with_count2 = wc2.top(x_max, lambda(k,v): v)\n",
    "top_freq2 = float(top_words_with_count2[0][1])\n",
    "freq2 = np.array([(occurrences / top_freq2) for (word, occurrences) in top_words_with_count2])\n",
    "\n",
    "print freq2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_max_display = 100\n",
    "\n",
    "xs = np.arange(1, x_max_display+1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xs, y[:x_max_display], 'r', label='ideal (Zipf)')\n",
    "plt.plot(xs, freq1[:x_max_display], 'b', label='Shakespeare')\n",
    "plt.plot(xs, freq2[:x_max_display], 'g', label='Dickens')\n",
    "plt.legend()\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('Relative frequency')\n",
    "plt.title('Relative word frequency in different text corpora')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
