{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a><div align=\"center\">This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</div>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute $\\pi$ with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to compute an approximation to the constant \"Pi\" using a Map/Reduce job.  The original source can be found at https://github.com/apache/spark/blob/master/examples/src/main/python/pi.py\n",
    "\n",
    "The computation is based on a quasi-Monte Carlo method: we \"sample\" the area of the unit circle (which is exactly \"Pi\") by counting the number of randomly-generated coordinate pairs that fall within the unit circle.  A detailed explanation of the method can be found [here](http://mathfaculty.fullerton.edu/mathews/n2003/montecarlopimod.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python-only solution\n",
    "\n",
    "As a starter, we can formulate a solution to the problem in pure Python using the built-in `map`, `filter`, `reduce` functions.  This leads to a straightforward generalisation for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of iterations (constant)\n",
    "N = 16*1000*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. pick $N$ random points in the square $[0,+1] \\times [0,+1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following function picks a point in the unit square by generating the $x$ and $y$ coordinates uniformly at random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python's uniform random number generator\n",
    "from random import random\n",
    "\n",
    "def point(_):  ## still need 1 argument for `map` below\n",
    "    \"\"\"A random point in the unit square.\"\"\"\n",
    "    x = random()\n",
    "    y = random()\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get an array of $N$ random points by just applying this function to any list of the correct size (note that `point` still takes a parameter, and ignores it!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.18 s, sys: 998 ms, total: 6.18 s\n",
      "Wall time: 6.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "points = map(point, range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.42855644154377803, 0.13429609611151172),\n",
       " (0.8917763049620622, 0.04446035994226749),\n",
       " (0.6524320657951928, 0.983059026998789),\n",
       " (0.41781473006532777, 0.8395036432321322),\n",
       " (0.8542528152501607, 0.3561265631343048)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. count the number $P$ of points that fall into the unit circle $\\{ (x,y) | x^2+y^2 < 1 \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply `filter` and keep only the points that lie withing the unit circle -- the length of the list so obtained will be $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.72 s, sys: 60.6 ms, total: 4.79 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "points_in_unit_circle = filter((lambda (x,y): (x**2 + y**2) < 1), points)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = len(points_in_unit_circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. for large enough $N$, the ratio $P/N$ approximates the area of a quarter of the unit circle, i.e. $\\pi/4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141209\n"
     ]
    }
   ],
   "source": [
    "pi = 4.0 * P / N\n",
    "\n",
    "print(\"Pi is roughly %f\" % pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since also Spark RRDs provide `map` and `filter` methods, we can straightforwardly translate the above Python solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A constant that is unique to Spark is the number of partitions, which controls the gramularity of parallelism (more partitions, smaller chunks of work sent to the executors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partitions = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process data in Spark, we have to inject it in the system using `SparkContext.parallelize()` and get an RDD back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 863 ms, sys: 140 ms, total: 1 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# inject an array of size N into Spark\n",
    "dummy = sc.parallelize(range(N), partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs provide a `.map()` method that creates a new RDD by applying the given function elementwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 326 ms, sys: 4.68 ms, total: 330 ms\n",
      "Wall time: 333 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create collection of random points\n",
    "points = dummy.map(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the `.filter()` method creates a new RDD by copying only those values on which the supplied predicate is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 483 ms, sys: 68.5 ms, total: 552 ms\n",
      "Wall time: 554 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# keep only those in unit circle\n",
    "points_in_unit_circle = points.filter(lambda (x,y): (x**2 + y**2) < 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `.count()` method returns the number of elements in a RDD (like Python's built-in `len()`).  \n",
    "\n",
    "Note that `.count()` is an **action** so it triggers actual computation of the DAG we have been building so far -- almost all the work that Spark does is concentrated in this single line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 0 ns, total: 10.5 ms\n",
      "Wall time: 7.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# count the latter\n",
    "P = points_in_unit_circle.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141718\n"
     ]
    }
   ],
   "source": [
    "pi = 4.0 * P / N\n",
    "\n",
    "print(\"Pi is roughly %f\" % pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Map/Reduce approach\n",
    "\n",
    "\n",
    "This algorithm can be realized via a map/reduce scheme by using mappers that take no input and output 0 or 1 depending on whether a single randomly-generated point landed inside the circle or not.  Then the reducers simply add all the results.\n",
    "\n",
    "This solution does not strictly depend on Spark-only features and could in principle run on any Map/Reduce engine.  On the other hand, all the worker is done on the mappers and the reduce step does not parallelize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's standard [random()](https://docs.python.org/2/library/random.html#module-random) function generates a floating-point number uniformly in the semi-open range [0.0, 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we would need to initialize a Spark context in order to perform initialization.  However, this is automatically done in this IPython installation, so we skip this part. (Only *one* SparkContext object can be alive at the same time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sc = SparkContext(appName=\"PythonPi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two parameters `partitions` and `n` control the number of mappers and the number of repetitions that Spark is going to execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partitions = 16\n",
    "n = 1000000 * partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper function just generates a random point in the square [0, +1] x [0, +1] and then outputs 1 if it fell inside the unit circle and 0 if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randomly_in_unit_circle(_):\n",
    "    x = random()\n",
    "    y = random()\n",
    "    if (x**2 + y**2) < 1:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *reduce* function just adds all these 0's and 1's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add(a,b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the total number of positive samples is found by applying map/reduce to a list of length `n`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 830 ms, sys: 137 ms, total: 967 ms\n",
      "Wall time: 6.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count = (sc.parallelize(range(1, n + 1), partitions)\n",
    "         .map(randomly_in_unit_circle)\n",
    "         .reduce(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the area of the circle is computed as the number of positive samples divided by the area of the enclosing square (all samples land in the square); the factor `4` comes from the fact that we're sampling only the x>0, y>0 sector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.140334\n"
     ]
    }
   ],
   "source": [
    "pi = 4.0 * count / n\n",
    "\n",
    "print(\"Pi is roughly %f\" % pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally the SparkContext is stopped when the computations are done.  We don't do it here in order to be able to run more computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
